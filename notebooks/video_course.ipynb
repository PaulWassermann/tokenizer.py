{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build the GPT tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effects of tokenization on decoder LLM capabilities :\n",
    "- unability to spell words\n",
    "- unability to do basic arithmetics\n",
    "- unability to do basic string processing tasks\n",
    "- preference over some file format (e.g. YAML is better generated than JSON for GPT-2)\n",
    "- preference over language -> less occuring language in the tokenizer training dataset have a longer token representation, which means the information is less dense for a given input from the model perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Python documentation](https://docs.python.org/3/library/functions.html#ord), `ord` is the inverse function of `chr`. It means that its input are single Unicode characters, and its outputs are the associated Unicode code point (i.e integers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128539"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"üòõ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12362,\n",
       " 12399,\n",
       " 12424,\n",
       " 12358,\n",
       " 65281,\n",
       " 32,\n",
       " 73,\n",
       " 116,\n",
       " 32,\n",
       " 109,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 115,\n",
       " 32,\n",
       " 39,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 39,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 74,\n",
       " 97,\n",
       " 112,\n",
       " 97,\n",
       " 110,\n",
       " 101,\n",
       " 115,\n",
       " 101,\n",
       " 32,\n",
       " 58,\n",
       " 41]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"„Åä„ÅØ„Çà„ÅÜÔºÅ It means 'hello' in Japanese :)\"\n",
    "[ord(char) for char in my_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode code points can't be used as such because they are too many. Instead, we can resort to using a unicode encoding such as [UTF-8](https://fr.wikipedia.org/wiki/UTF-8) which encodes each unicode symbol on 1 to 4 bytes, meaning any sentence would be representing by a sequence of integers ranging from 0 to 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Further readings* : \n",
    "\n",
    "#### [A programmer's introduction to Unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/) and [UTF-8 everywhere](https://utf8everywhere.org/)\n",
    "\n",
    "Unicode first draft proposal dates back to 1988, published py Joseph D. Becker. One interesting feature of UTF-8 is that it is *endianness independent* (independ of byte order).\n",
    "\n",
    "*UTF* (as in *UTF-8*) stands for *Unicode Transformation Format*. The Unicode *codespace* is the set of all possible Unicode *codepoints*. The codepoints are the integer identifiers for each assigned symbols, with a potential range from 0 to 1,114,111 (with only approximately 12% assigned). Each code point could be represented with 4 bytes, but this reveals to be wasteful for various reasons, one of them being that most used Unicode symbols have codepoints inferior to 65,536. The 3 main Unicode encodings are :\n",
    "- **UTF-8**: a *variable-length* encoding where symbols can be represented with 1 to 4 bytes depending on their codepoint\n",
    "- **UTF-16**: a *variable-length* encoding where symbols can be represented with 2 to 4 bytes depending on their codepoint\n",
    "- **UTF-32**: a *fixed-length* encoding where symbols are represented with exactly 4 bytes\n",
    "\n",
    "We call *code unit* the minimal number of bits that can be used to represent a unit of encoded text (8 bits for UTF-8, 16 for UTF-16 and 32 for UTF-32).\n",
    "\n",
    "The main advantage of variable-length encodings is memory saving, and their main drawback is that they are slower to compute.\n",
    "\n",
    "For UTF-8 (and UTF-16 likewise), how can we distinguish between single and multi-bytes sequences ? Sequences of different bytes-length use unique prefixes:\n",
    "\n",
    "| UTF-8 (binary)                      | Code point (binary)   | Range                               |\n",
    "|-------------------------------------|-----------------------|-------------------------------------|\n",
    "| 0xxxxxxx                            | xxxxxxx               | U+0000‚ÄìU+007F (0-127)               |\n",
    "| 110xxxxx 10yyyyyy                   | xxxxxyyyyyy           | U+0080‚ÄìU+07FF (128-2,047)           |\n",
    "| 1110xxxx 10yyyyyy 10zzzzzz          | xxxxyyyyyyzzzzzz      | U+0800‚ÄìU+FFFF (2,048-65,535)        |\n",
    "| 11110xxx 10yyyyyy 10zzzzzz 10wwwwww | xxxyyyyyyzzzzzzwwwwww | U+10000‚ÄìU+10FFFF (65,536-1,114,111) |\n",
    "\n",
    "Unicode symbols can be dynamically composed to form new characters (for instance, diacritics can be composed with letters). However, \"composed\" characters can have different representations resulting in the same rendering, and some composed characters even have their own codepoint in the Unicode standard! Hence the need for *normalization forms*, which reorder bytes according to certain rules, preserving the rendered characters but allowing for byte-to-byte comparison.\n",
    "\n",
    "A *grapheme cluster* is a \"user-perceived character\"; this notion is particularly useful in the context of text editors, since they are used for cursor placement and text selection boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[227,\n",
       " 129,\n",
       " 138,\n",
       " 227,\n",
       " 129,\n",
       " 175,\n",
       " 227,\n",
       " 130,\n",
       " 136,\n",
       " 227,\n",
       " 129,\n",
       " 134,\n",
       " 239,\n",
       " 188,\n",
       " 129,\n",
       " 32,\n",
       " 73,\n",
       " 116,\n",
       " 32,\n",
       " 109,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 115,\n",
       " 32,\n",
       " 39,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 39,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 74,\n",
       " 97,\n",
       " 112,\n",
       " 97,\n",
       " 110,\n",
       " 101,\n",
       " 115,\n",
       " 101,\n",
       " 32,\n",
       " 58,\n",
       " 41]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_utf8_string = my_string.encode(\"utf-8\")\n",
    "list(my_utf8_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte pair encoding\n",
    "\n",
    "Why do we need byte pair encoding ? Language models have a limited context window in which a token can attend to all previous tokens, and utf-8 representation of text strings are necessarily at least as long as the text's number of characters. Byte pair encoding allows to find recurring patterns in a given \"training\" corpus, in order to add those patterns to the vocabulary, and thus reducing the number of tokens required to represent an average piece of text. In other words, byte pair encodind allows for statisticlly efficient information compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenizer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
